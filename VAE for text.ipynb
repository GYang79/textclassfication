{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfortunately , it doesn't work very well, and I guess it is because features it captures are too simple, and RNN or similiar method should be the only way to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_2 = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def sample(self,mu,logvar): # reparametrize\n",
    "        eps = Variable(torch.randn(mu.size()))# generate the standard guass rv having the same size with mu\n",
    "        eps = eps.cuda()\n",
    "        std = torch.exp(logvar/2.0)\n",
    "        return mu+ eps * std   # Using reparametrization for backprop\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderrnn(encoder):\n",
    "    def __init__(self, input_size=embedding_size, hidden_size=70, output_size=latent, n_layers=2, bidirectional=True):\n",
    "        super(encoderrnn, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(max_word, embedding_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix_2, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size , hidden_size, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True,dropout=0.1)\n",
    "        self.linearout = nn.Linear(hidden_size*2*maxq, output_size*2)\n",
    "    def forward(self, input):\n",
    "        h_embedding = self.embedding(input)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        out = h_lstm.contiguous().view(-1,self.hidden_size*2*maxq)\n",
    "        out = self.linearout(out)\n",
    "        mu,logvar = torch.chunk(out,2,dim=1)\n",
    "        z = torch.tensor(self.sample(mu, logvar))\n",
    "        return mu,logvar,z,h_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderrnn(nn.Module):\n",
    "    def __init__(self, input_size=latent, hidden_size=70, \n",
    "                 output_size=embedding_size, nlayers=3, dropout=0.1):\n",
    "        super(decoderrnn,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(input_size, self.hidden_size*maxq)\n",
    "        self.lstm = nn.LSTM(hidden_size,output_size, num_layers= nlayers, bidirectional=True)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.linear1(z)\n",
    "        out = out.contiguous().view(-1,maxq,self.hidden_size)\n",
    "        out,_ = self.lstm(out)\n",
    "\n",
    "        out = out[:,:,:self.output_size]+out[:,:,self.output_size:]\n",
    "        return out\n",
    "    \n",
    "    def generate(self,rv):\n",
    "        with torch.no_grad():\n",
    "            embed = self.forward(rv)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(vae,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,inputs):\n",
    "        mu, v, z,embed = self.encoder(inputs)\n",
    "        gen = self.decoder(z)\n",
    "        return mu, v, gen,embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(mu,v):\n",
    "    out = torch.sum(torch.exp(2*v),1)+ torch.sum(torch.pow(mu,2),1)-torch.log(torch.sum(torch.exp(2*v),1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = encoderrnn()\n",
    "de = decoderrnn()\n",
    "model2 = vae(en,de)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model2.parameters(),lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfoldx = torch.tensor(train_X[train_y==1], dtype=torch.long).to(device)\n",
    "train = torch.utils.data.TensorDataset(trainfoldx)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    grand_loss = 0\n",
    "    for  batch_idx, (x_batch) in enumerate(train_loader):\n",
    "            #forward\n",
    "        mu, v, gen,embedding = model2(x_batch[0])\n",
    "        KL = compute_KL(mu,v)\n",
    "        KL = KL.mean()\n",
    "        Fit =torch.max(torch.abs(gen-embedding),2)[0].mean()*150\n",
    "        loss = Fit - KL\n",
    "            \n",
    "            #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' \n",
    "                   %(epoch+1, n_epochs, batch_idx, len(train_loader), loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
